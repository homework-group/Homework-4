---
title: "Exercise 4 - 1"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Clustering and PCA

I run both clustering and PCA on the 11 properties of wine data.

I run clustering first.
```{r , include=FALSE}
library(ggplot2)
library(LICORS)  
library(foreach)
library(mosaic)
library(tidyverse)

wine = read.csv("~/Desktop/Data Mining/Exercise4-1/wine.csv", header=TRUE)
```

```{r, include=FALSE}
#### clustering

x = wine[,1:11]
x = scale(x,center = TRUE, scale = TRUE)

mu = attr(x,"scaled:center")
sigma = attr(x,"scaled:scale")
```

I use kmeans++ method to clustering.
```{r}
# divide the data in 2 clusters to distinguish red from white
clust1 = kmeanspp(x,k = 2, nstart = 50)
```

```{r, echo=FALSE}
# white and red
x1 = which(clust1$cluster == 1)
x2 = which(clust1$cluster == 2)
table(wine[x1,13])
table(wine[x2,13])
```

Regardless small error, clustering can distinguish white and red generally. The wines belong to the cluster 1 is white generally and the wines belobg to the cluster 2 is red in general.
However it is hard to sort the higher frome the lower quality wines, the result is below:
```{r, echo= FALSE}
# distinguish quality
table(wine[x1,12])
table(wine[x2,12])
```

As we can see, the quality is evenly distributed.

Next I use PCA to divide the wine data.

```{r, include=FALSE}
#### PCA
z = wine[,1:11]
pairs(z)
pc1 = prcomp(z, scale.=TRUE)
summary(pc1)
loadings = pc1$rotation
scores = pc1$x
```

See difference between white wine and red wine, and it can easily tell that PC1 is the best way to distinguish between white and red wine.



```{r, echo=FALSE}
# white and red
qplot(scores[,1], scores[,2], color=wine$color, xlab='Component 1', ylab='Component 2')
qplot(scores[,1], scores[,3], color=wine$color, xlab='Component 1', ylab='Component 3')
qplot(scores[,2], scores[,3], color=wine$color, xlab='Component 2', ylab='Component 3')
```

To distingusih between quality.
```{r,echo=FALSE}
#quality
qplot(scores[,1], scores[,2], color=wine$quality, xlab='Component 1', ylab='Component 2')
qplot(scores[,1], scores[,3], color=wine$quality, xlab='Component 1', ylab='Component 3')
qplot(scores[,1], scores[,4], color=wine$quality, xlab='Component 1', ylab='Component 4')
qplot(scores[,1], scores[,5], color=wine$quality, xlab='Component 1', ylab='Component 5')
qplot(scores[,1], scores[,6], color=wine$quality, xlab='Component 1', ylab='Component 6')
qplot(scores[,1], scores[,7], color=wine$quality, xlab='Component 1', ylab='Component 7')
qplot(scores[,1], scores[,8], color=wine$quality, xlab='Component 1', ylab='Component 8')
qplot(scores[,1], scores[,9], color=wine$quality, xlab='Component 1', ylab='Component 9')
qplot(scores[,1], scores[,10], color=wine$quality, xlab='Component 1', ylab='Component 10')
qplot(scores[,1], scores[,11], color=wine$quality, xlab='Component 1', ylab='Component 11')

```

As we can see above, it can not sort the wine according to quality no matter what component it is.

### Question

1.Which dimensionality reduction technique makes more sense to you for this data? 

I think both dimensionality reduction technique makes sense, but PCA makes more senses. Because PCA can show the part that is overlapped, it is more precise.


2.Does this technique also seem capable of sorting the higher from the lower quality wines?

I think both the technique can not sort the higher from the lower quality wines, as we can see above.

# HW4-2

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r,include=FALSE}
rm(list=ls())
library(ggplot2)
library(LICORS) 
library(foreach)
library(mosaic)
library(cluster)
```
```{r,include=FALSE}
mkt = read.csv("~/Documents/GitHub/ECO395M/data/social_marketing.csv")
mkt$total <- rowSums(mkt[,2:37])
mkt$spam_rate<-mkt$spam/mkt$total
mkt$adult_rate<-mkt$adult/mkt$total
```
Overall view about data.



```{r,echo=FALSE}
ggplot(data = mkt) + 
  geom_point(mapping = aes(x = adult_rate, y = spam, size = total))+
  labs(x="adult rate", y="spam #", title="adult and spam tweets posting", size="total tweets")
```
Categorize as spam-bot if number of spam tweets>1, as adult-bot if adult rate > 20%, and remove them.
We are relatively strict in removing users who post spams but more tolerent if users post adult content. This is because adult content is an option of interest for normal adult users, but normal users are not likely to post spams.
```{r,include=FALSE}
mkt <- mkt[mkt$spam<1,]
mkt <- mkt[mkt$adult_rate<0.2,]
mkt$total<-NULL
mkt$spam_rate<-NULL
mkt$adult_rate<-NULL
mkt$spam<-NULL
```
# Clustering
Use k++ method to select k
```{r,echo=FALSE,error=FALSE}
X = scale(mkt[,-1]) # cluster on measurables
k_grid = seq(2, 20, by=1)
SSE_grid = foreach(k = k_grid, .combine='c') %do% {
  cluster_k = kmeans(X, k, nstart=50)
  cluster_k$tot.withinss
}
plot(k_grid, SSE_grid)
```
We choose k=10 for clustering.
```{r,include=FALSE}
X = mkt[,-1]
X = scale(X, center=TRUE, scale=TRUE)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
# Run k-means with 10 clusters and 25 starts (restart 25 times)
clust1 = kmeans(X, 10, nstart=25)
# Below are the center of 10 clusters
for (i in 1:10){
  cat("cluster ",i)
  cat("      ")
  cat("cluster size = ",length(which(clust1$cluster == i)))
  cat("\n\n")
  cl1<-as.data.frame(clust1$center[i,]*sigma + mu)
  cl1$category<-rownames(cl1)
  cl1 <- cl1[order(-cl1$`clust1$center[i, ] * sigma + mu`),]
  colnames(cl1)[1] <- "cluster center"
  cl1$category<-NULL
  print(head(cl1, n=4))
  cat("--------------------")
  cat("\n")
}
```
We can see that the largest clusters includes people with interests in health&fitness, sports-fandom,cooking, college/univ, news&politics, dating&photosharing.

#Pca to analyze the most prominent features of users
```{r,echo=FALSE}
pc = prcomp(X, scale=TRUE, rank=8)
summary(pc)
plot(pc)
loadings = pc$rotation
scores = pc$x
loadings = pc$rotation
for (i in 1:8){
print(loadings[,1] %>% sort %>% tail(3))
}
```
The result seems average and is a group of correlated interests.

#q3

Find some interesting association rules for these shopping baskets
Pick your own thresholds for lift and confidence
```{r setup, include=FALSE}
library(tidyverse)
library(arules)  # has a big ecosystem of packages built around it
library(arulesViz)
library(reshape)
```


```{r cars,include=FALSE}
groceries = read.csv("../groceries.csv",header=FALSE)
groceries$buyers <- seq.int(nrow(groceries))
grocery_raw <-melt(groceries, id =c("buyers"))
grocery_raw<- grocery_raw[,-2]
grocery_raw <- grocery_raw[-which(grocery_raw$value == ""), ]
colnames(grocery_raw)[2] <- "item"
grocery_raw <- as.data.frame(grocery_raw)
str(grocery_raw)
summary(grocery_raw)
```

Show the top 20 frequent items in the baskets using the bar chart, and the bar chart shows the basic scene of transactions.
```{r pressure, echo=FALSE}
grocery_raw$item %>%
  summary(maxsum=Inf) %>%
  sort(decreasing=TRUE) %>%
  head(20) %>%
  barplot(las=2, cex.names=0.6)
```

```{r,echo=FALSE,include=FALSE}
grocery_raw$buyers = factor(grocery_raw$buyers)
groceries = split(x=grocery_raw$item, f=grocery_raw$buyers)
groceries[[1]]
groceries[[2]]
```

```{r,echo=FALSE,include=FALSE}
groceries = lapply(groceries, unique)
grocerytrans = as(groceries, "transactions")
summary(grocerytrans)
```
In this part, we choose the support as 0.1, thus the probability of transactions that contain all these items in the basket will be higher than 0.1. There are around 15,000 transactions, thus the support 0.1 will give more than 1500 times combinations.
For the confidence, we choose 0.6, since confidence rate should be larger than support rate. Lower confidence rate will give more association. The graph shows that when confidence rate is lower than 0.6, the number of association will not go up as the confidence rate decreases. Hence, we think 0.6 is the best confidence rate to choose.
```{r,echo=FALSE,include=FALSE}
groceryrules = apriori(grocerytrans, 
                     parameter=list(support=.01, confidence=.06))
```

```{r,echo=FALSE,include=FALSE}
inspect(groceryrules)
inspect(subset(groceryrules, lift > 5))
inspect(subset(groceryrules, confidence > 0.06))
inspect(subset(groceryrules, lift > 10 & confidence > 0.06))
```

```{r,echo=FALSE}
plot(groceryrules)
```

```{r,echo=FALSE,include=FALSE}
inspect(subset(groceryrules, support > 0.01))
inspect(subset(groceryrules, confidence > 0.6))
```
We can visualize the association rules through network graph. The larger the label size, the more frequent this item appeared in a transaction, which is another representation of the barplot. The dark the color of the edge, the higher the lift of the association, which corresponded to what I found. 
We use the graph to show the association rules. In the following graph, the label size represents the frequency of the item, the color degree represents the lift of the association.
The dark the color is, the higher lift is.
```{r,echo=FALSE,include=FALSE}
sub1 = subset(groceryrules, subset=confidence > 0.06 & support > 0.01)
summary(sub1)
plot(sub1, method='graph')
plot(head(sub1, 100, by='lift'), method='graph')
saveAsGraph(sub1, file = "grocery.graphml")
```

---
title: "Exercise 4 - 1"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Clustering and PCA

I run both clustering and PCA on the 11 properties of wine data.

I run clustering first.
```{r , include=FALSE}
library(ggplot2)
library(LICORS)  
library(foreach)
library(mosaic)
library(tidyverse)

wine = read.csv("~/Desktop/Data Mining/Exercise4-1/wine.csv", header=TRUE)
```

```{r, include=FALSE}
#### clustering

x = wine[,1:11]
x = scale(x,center = TRUE, scale = TRUE)

mu = attr(x,"scaled:center")
sigma = attr(x,"scaled:scale")
```

I use kmeans++ method to clustering.
```{r}
# divide the data in 2 clusters to distinguish red from white
clust1 = kmeanspp(x,k = 2, nstart = 50)
```

```{r, echo=FALSE}
# white and red
x1 = which(clust1$cluster == 1)
x2 = which(clust1$cluster == 2)
table(wine[x1,13])
table(wine[x2,13])
```

Regardless small error, clustering can distinguish white and red generally. The wines belong to the cluster 1 is white generally and the wines belobg to the cluster 2 is red in general.
However it is hard to sort the higher frome the lower quality wines, the result is below:
```{r, echo= FALSE}
# distinguish quality
table(wine[x1,12])
table(wine[x2,12])
```

As we can see, the quality is evenly distributed.

Next I use PCA to divide the wine data.

```{r, include=FALSE}
#### PCA
z = wine[,1:11]
pairs(z)
pc1 = prcomp(z, scale.=TRUE)
summary(pc1)
loadings = pc1$rotation
scores = pc1$x
```

See difference between white wine and red wine, and it can easily tell that PC1 is the best way to distinguish between white and red wine.



```{r, echo=FALSE}
# white and red
qplot(scores[,1], scores[,2], color=wine$color, xlab='Component 1', ylab='Component 2')
qplot(scores[,1], scores[,3], color=wine$color, xlab='Component 1', ylab='Component 3')
qplot(scores[,2], scores[,3], color=wine$color, xlab='Component 2', ylab='Component 3')
```

To distingusih between quality.
```{r,echo=FALSE}
#quality
qplot(scores[,1], scores[,2], color=wine$quality, xlab='Component 1', ylab='Component 2')
qplot(scores[,1], scores[,3], color=wine$quality, xlab='Component 1', ylab='Component 3')
qplot(scores[,1], scores[,4], color=wine$quality, xlab='Component 1', ylab='Component 4')
qplot(scores[,1], scores[,5], color=wine$quality, xlab='Component 1', ylab='Component 5')
qplot(scores[,1], scores[,6], color=wine$quality, xlab='Component 1', ylab='Component 6')
qplot(scores[,1], scores[,7], color=wine$quality, xlab='Component 1', ylab='Component 7')
qplot(scores[,1], scores[,8], color=wine$quality, xlab='Component 1', ylab='Component 8')
qplot(scores[,1], scores[,9], color=wine$quality, xlab='Component 1', ylab='Component 9')
qplot(scores[,1], scores[,10], color=wine$quality, xlab='Component 1', ylab='Component 10')
qplot(scores[,1], scores[,11], color=wine$quality, xlab='Component 1', ylab='Component 11')

```

As we can see above, it can not sort the wine according to quality no matter what component it is.

### Question

1.Which dimensionality reduction technique makes more sense to you for this data? 

I think both dimensionality reduction technique makes sense, but PCA makes more senses. Because PCA can show the part that is overlapped, it is more precise.


2.Does this technique also seem capable of sorting the higher from the lower quality wines?

I think both the technique can not sort the higher from the lower quality wines, as we can see above.

# HW4-2

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r,include=FALSE}
rm(list=ls())
library(ggplot2)
library(LICORS) 
library(foreach)
library(mosaic)
library(cluster)
```
```{r,include=FALSE}
mkt = read.csv("~/Documents/GitHub/ECO395M/data/social_marketing.csv")
mkt$total <- rowSums(mkt[,2:37])
mkt$spam_rate<-mkt$spam/mkt$total
mkt$adult_rate<-mkt$adult/mkt$total
```
Overall view about data.



```{r,echo=FALSE}
ggplot(data = mkt) + 
  geom_point(mapping = aes(x = adult_rate, y = spam, size = total))+
  labs(x="adult rate", y="spam #", title="adult and spam tweets posting", size="total tweets")
```
Categorize as spam-bot if number of spam tweets>1, as adult-bot if adult rate > 20%, and remove them.
We are relatively strict in removing users who post spams but more tolerent if users post adult content. This is because adult content is an option of interest for normal adult users, but normal users are not likely to post spams.
```{r,include=FALSE}
mkt <- mkt[mkt$spam<1,]
mkt <- mkt[mkt$adult_rate<0.2,]
mkt$total<-NULL
mkt$spam_rate<-NULL
mkt$adult_rate<-NULL
mkt$spam<-NULL
```
# Clustering
Use k++ method to select k
```{r,echo=FALSE,error=FALSE}
X = scale(mkt[,-1]) # cluster on measurables
k_grid = seq(2, 20, by=1)
SSE_grid = foreach(k = k_grid, .combine='c') %do% {
  cluster_k = kmeans(X, k, nstart=50)
  cluster_k$tot.withinss
}
plot(k_grid, SSE_grid)
```
We choose k=10 for clustering.
```{r,include=FALSE}
X = mkt[,-1]
X = scale(X, center=TRUE, scale=TRUE)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
# Run k-means with 10 clusters and 25 starts (restart 25 times)
clust1 = kmeans(X, 10, nstart=25)
# Below are the center of 10 clusters
for (i in 1:10){
  cat("cluster ",i)
  cat("      ")
  cat("cluster size = ",length(which(clust1$cluster == i)))
  cat("\n\n")
  cl1<-as.data.frame(clust1$center[i,]*sigma + mu)
  cl1$category<-rownames(cl1)
  cl1 <- cl1[order(-cl1$`clust1$center[i, ] * sigma + mu`),]
  colnames(cl1)[1] <- "cluster center"
  cl1$category<-NULL
  print(head(cl1, n=4))
  cat("--------------------")
  cat("\n")
}
```
We can see that the largest clusters includes people with interests in health&fitness, sports-fandom,cooking, college/univ, news&politics, dating&photosharing.

#Pca to analyze the most prominent features of users
```{r,echo=FALSE}
pc = prcomp(X, scale=TRUE, rank=8)
summary(pc)
plot(pc)
loadings = pc$rotation
scores = pc$x
loadings = pc$rotation
for (i in 1:8){
print(loadings[,1] %>% sort %>% tail(3))
}
```
The result seems average and is a group of correlated interests.
